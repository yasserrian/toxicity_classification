# toxicity_classification

# Introduction:

Aujourd’hui, les plateformes numériques sont devenues des lieux principales pour échanger et partager des idées. Cependant, ces espaces sont souvent envahis par des comportements négatifs, comme les commentaires toxiques : menaces, insultes, obscénités, ou discours haineux ciblant une identité. Ces interactions nocives découragent les gens de participer aux discussions et peuvent même les pousser à complètement abandonner certains espaces en ligne.
Le problème, c’est que les outils actuels de modération, comme ceux proposés par Perspective API, ne sont pas parfaits. Bien qu’ils soient capables de détecter certains types de toxicité, ils manquent de précision et de flexibilité. Par exemple, ils ne permettent pas aux plateformes de décider quels types de contenu toxique elles veulent détecter (comme tolérer les jurons, mais pas les attaques personnelles). Résultat : les plateformes ont du mal à modérer efficacement tout en laissant suffisamment de liberté aux utilisateurs pour qu’ils s’expriment.
C’est là qu’intervient ce projet. L’idée est de créer un modèle capable de détecter plusieurs types de toxicité avec plus de précision et de s’adapter aux besoins spécifiques des plateformes. Un tel modèle pourrait aider à mieux gérer les comportements nuisibles en ligne, tout en encourageant des discussions plus respectueuses et ouvertes. L’objectif final : rendre les espaces numériques plus sûrs, inclusifs et propices à de vrais échanges d’idées.
